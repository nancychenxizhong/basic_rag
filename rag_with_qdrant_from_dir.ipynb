{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "be881a6d",
      "metadata": {
        "collapsed": true,
        "id": "be881a6d",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "# Retrieval Augmented Generation (RAG) with OpenAI / Claude and Qdrant\n",
        "\n",
        "Based on basic RAG example from [qdrant sample use cases](https://qdrant.tech/documentation/examples/)\n",
        "\n",
        "In the ever-evolving landscape of AI, the consistency and reliability of Large Language Models (LLMs) remain a challenge. While these models can understand statistical relationships between words, they often fail to provide accurate factual responses. Because their internal knowledge may not be accurate, outputs can range from spot-on to nonsensical. Retrieval Augmented Generation (RAG) is a framework designed to bolster the accuracy of LLMs by grounding them in external knowledge bases. In this example, we'll demonstrate a streamlined  implementation of the RAG pipeline using only Qdrant and OpenAI SDKs. By harnessing Flag embedding's power, we can bypass additional frameworks' overhead.\n",
        "    \n",
        "This example assumes you understand the architecture necessary to carry out RAG. If this is new to you, please look at some introductory readings:\n",
        "* [Retrieval-Augmented Generation: To add knowledge](https://eugeneyan.com/writing/llm-patterns/#retrieval-augmented-generation-to-add-knowledge)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb044259",
      "metadata": {
        "id": "bb044259"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Let's start setting up all the pieces to implement the RAG pipeline. We will only use Qdrant and OpenAI SDKs, without any third-party libraries.\n",
        "\n",
        "### Preparing the environment\n",
        "\n",
        "We need just a few dependencies to implement the whole application, so let's start with installing the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4ce9f81b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:32.977456Z",
          "start_time": "2023-09-27T10:06:30.203757Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ce9f81b",
        "outputId": "613d4173-67d9-4f00-fc5c-120d43790840",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting qdrant-client\n",
            "  Downloading qdrant_client-1.8.2-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting fastembed\n",
            "  Downloading fastembed-0.2.5-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.14.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting grpcio>=1.41.0 (from qdrant-client)\n",
            "  Downloading grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client)\n",
            "  Downloading grpcio_tools-1.62.1-cp311-cp311-macosx_10_10_universal2.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: httpx>=0.20.0 in ./.venv/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.27.0)\n",
            "Collecting numpy>=1.21 (from qdrant-client)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in ./.venv/lib/python3.11/site-packages (from qdrant-client) (2.6.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in ./.venv/lib/python3.11/site-packages (from qdrant-client) (2.2.1)\n",
            "Collecting huggingface-hub<0.21,>=0.20 (from fastembed)\n",
            "  Using cached huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting loguru<0.8.0,>=0.7.2 (from fastembed)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting onnx<2.0.0,>=1.15.0 (from fastembed)\n",
            "  Downloading onnx-1.16.0-cp311-cp311-macosx_10_15_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime<2.0.0,>=1.17.0 (from fastembed)\n",
            "  Downloading onnxruntime-1.17.1-cp311-cp311-macosx_11_0_universal2.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.31 in ./.venv/lib/python3.11/site-packages (from fastembed) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.16.0,>=0.15.1 in ./.venv/lib/python3.11/site-packages (from fastembed) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.66 in ./.venv/lib/python3.11/site-packages (from fastembed) (4.66.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai) (4.3.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.11/site-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Collecting protobuf<5.0dev,>=4.21.6 (from grpcio-tools>=1.41.0->qdrant-client)\n",
            "  Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (65.5.0)\n",
            "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub<0.21,>=0.20->fastembed) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<0.21,>=0.20->fastembed) (2024.3.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<0.21,>=0.20->fastembed) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<0.21,>=0.20->fastembed) (24.0)\n",
            "Collecting coloredlogs (from onnxruntime<2.0.0,>=1.17.0->fastembed)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flatbuffers (from onnxruntime<2.0.0,>=1.17.0->fastembed)\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Collecting sympy (from onnxruntime<2.0.0,>=1.17.0->fastembed)\n",
            "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic>=1.10.8->qdrant-client) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in ./.venv/lib/python3.11/site-packages (from pydantic>=1.10.8->qdrant-client) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3.0,>=2.31->fastembed) (3.3.2)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->fastembed)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting mpmath>=0.19 (from sympy->onnxruntime<2.0.0,>=1.17.0->fastembed)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading qdrant_client-1.8.2-py3-none-any.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.2/223.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading fastembed-0.2.5-py3-none-any.whl (26 kB)\n",
            "Downloading openai-1.14.3-py3-none-any.whl (262 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.9/262.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
            "\u001b[?25hDownloading grpcio_tools-1.62.1-cp311-cp311-macosx_10_10_universal2.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
            "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl (20.6 MB)\n",
            "Downloading onnx-1.16.0-cp311-cp311-macosx_10_15_x86_64.whl (15.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.17.1-cp311-cp311-macosx_11_0_universal2.whl (14.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, flatbuffers, sympy, protobuf, portalocker, numpy, loguru, hyperframe, humanfriendly, hpack, grpcio, onnx, huggingface-hub, h2, grpcio-tools, coloredlogs, openai, onnxruntime, qdrant-client, fastembed\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.22.2\n",
            "    Uninstalling huggingface-hub-0.22.2:\n",
            "      Successfully uninstalled huggingface-hub-0.22.2\n",
            "Successfully installed coloredlogs-15.0.1 fastembed-0.2.5 flatbuffers-24.3.25 grpcio-1.62.1 grpcio-tools-1.62.1 h2-4.1.0 hpack-4.0.0 huggingface-hub-0.20.3 humanfriendly-10.0 hyperframe-6.0.1 loguru-0.7.2 mpmath-1.3.0 numpy-1.26.4 onnx-1.16.0 onnxruntime-1.17.1 openai-1.14.3 portalocker-2.8.2 protobuf-4.25.3 qdrant-client-1.8.2 sympy-1.12\n"
          ]
        }
      ],
      "source": [
        "!pip install qdrant-client fastembed openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aae4382a",
      "metadata": {
        "id": "aae4382a"
      },
      "source": [
        "[Qdrant](https://qdrant.tech) will act as a knowledge base providing the context information for the prompts we'll be sending to the LLM. There are various ways of running Qdrant, but we'll simply use the Docker container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e8f4456c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:34.283299Z",
          "start_time": "2023-09-27T10:06:32.980517Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8f4456c",
        "outputId": "3e483d21-aa32-4eed-e4a3-c451ff5efc96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "docker: Error response from daemon: Conflict. The container name \"/rag-openai-qdrant\" is already in use by container \"ef9c570f276697c0a7ae49104905792a2cde578c2a5506ad2ae496ed3da80102\". You have to remove (or rename) that container to be able to reuse that name.\n",
            "See 'docker run --help'.\n"
          ]
        }
      ],
      "source": [
        "!docker run -p \"6333:6333\" -p \"6334:6334\" --name \"rag-openai-qdrant\" --rm -d qdrant/qdrant:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a74c7a21",
      "metadata": {
        "id": "a74c7a21"
      },
      "source": [
        "### Creating the collection\n",
        "\n",
        "Qdrant [collection](https://qdrant.tech/documentation/concepts/collections/) is the basic unit of organizing your data. Each collection is a named set of points (vectors with a payload) among which you can search. After connecting to our running Qdrant container, we can check whether we already have some collections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2dd8966b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:36.242783Z",
          "start_time": "2023-09-27T10:06:34.289290Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "2dd8966b",
        "outputId": "f09d52a3-a2d7-46d4-b5c9-dd1555e64057"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nancyzzz/Documents/local_projects/claude_test/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CollectionsResponse(collections=[CollectionDescription(name='default_collection'), CollectionDescription(name='knowledge-base')])"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import qdrant_client\n",
        "\n",
        "qdrant_client = qdrant_client.QdrantClient(\"http://localhost:6333\", prefer_grpc=True)\n",
        "qdrant_client.get_collections()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f54205",
      "metadata": {
        "id": "23f54205"
      },
      "source": [
        "### Building the knowledge base\n",
        "\n",
        "Qdrant will use vector embeddings of our facts to enrich the original prompt with some context. Thus, we need to store the vector embeddings and the texts used to generate them. All our facts will have a JSON payload with a single attribute and look as follows:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"document\": \"Binary Quantization is a method of reducing the memory usage even up to 40 times!\"\n",
        "}\n",
        "```\n",
        "\n",
        "This structure is required by [FastEmbed](https://qdrant.github.io/fastembed/), a library that simplifies managing the vectors, as you don't have to calculate them on your own. It's also possible to use an existing collection, However, all the code snippets will assume this data structure. Adjust your examples to work with a different schema.\n",
        "\n",
        "FastEmbed will automatically create the collection if it doesn't exist. Knowing that we are set to add our documents to a collection, which we'll call `knowledge-base`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "43154775",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:36.692231Z",
          "start_time": "2023-09-27T10:06:36.245915Z"
        },
        "id": "43154775",
        "outputId": "ced66d31-4e0b-4d97-dfd0-f7c48844f5aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['69a21a6c4ed449f386ce2ff957b19052',\n",
              " '84f55a9325c84abab0dbfaf2dfbf675d',\n",
              " '65c8a7fd2f1a4afa96d432ee333101ce',\n",
              " 'bcaf626371b24898b53da2dd1994cf89',\n",
              " '3e72bb6ec5514a4289bcad8ad8d66615',\n",
              " '0606d89c2f974b39bf0db4723d3766c2',\n",
              " 'a952fd658c8448199a91ef90bd07ad03',\n",
              " '94cedf9f528d4531bcfd6fbf013d127f']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qdrant_client.add(\n",
        "    collection_name=\"knowledge-base\",\n",
        "    documents=[\n",
        "        \"Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\",\n",
        "        \"Docker helps developers build, share, and run applications anywhere — without tedious environment configuration or management.\",\n",
        "        \"PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.\",\n",
        "        \"MySQL is an open-source relational database management system (RDBMS). A relational database organizes data into one or more data tables in which data may be related to each other; these relations help structure the data. SQL is a language that programmers use to create, modify and extract data from the relational database, as well as control user access to the database.\",\n",
        "        \"NGINX is a free, open-source, high-performance HTTP server and reverse proxy, as well as an IMAP/POP3 proxy server. NGINX is known for its high performance, stability, rich feature set, simple configuration, and low resource consumption.\",\n",
        "        \"FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\",\n",
        "        \"SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.\",\n",
        "        \"The cron command-line utility is a job scheduler on Unix-like operating systems. Users who set up and maintain software environments use cron to schedule jobs (commands or shell scripts), also known as cron jobs, to run periodically at fixed times, dates, or intervals.\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34056b2f",
      "metadata": {},
      "source": [
        "### Building collection from url link and file dir\n",
        "`create_qdrant_collection_from_dir` function will re-create the collection if the name is the same. The default collection name is default_collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "51b18cc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import qdrant_retrieval\n",
        "# from qdrant_client import QdrantClient\n",
        "import importlib\n",
        "importlib.reload(qdrant_retrieval)\n",
        "from qdrant_retrieval import create_qdrant_collection_from_dir, retrieve_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0017bdd9",
      "metadata": {},
      "source": [
        "### Testing input data types"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "736754d0",
      "metadata": {},
      "source": [
        "URLs with .md - using requests to download file from the URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e2e6ea4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "create_qdrant_collection_from_dir(\n",
        "    dir_path= \n",
        "    [\"https://raw.githubusercontent.com/microsoft/flaml/main/README.md\",\n",
        "    \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\"],\n",
        "    text_types=[\"url\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28cdf25e",
      "metadata": {},
      "source": [
        "Text files or text file dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b64e239b",
      "metadata": {},
      "outputs": [],
      "source": [
        "create_qdrant_collection_from_dir(\n",
        "    dir_path=[\"txt_files/\"],\n",
        "    text_types=[\"txt\"],\n",
        "    collection_name=\"txt_collection\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da294fdc",
      "metadata": {},
      "source": [
        "PDF file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "e075205c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t{ ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t ...\n"
          ]
        },
        {
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_qdrant_collection_from_dir(\n\u001b[1;32m      2\u001b[0m     dir_path\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mdatasets/preprocessed_data/europarl/train.json\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      3\u001b[0m     text_types\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mjson\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      4\u001b[0m     collection_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mjson_collection\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/local_projects/claude_test/qdrant_retrieval.py:197\u001b[0m, in \u001b[0;36mcreate_qdrant_collection_from_dir\u001b[0;34m(dir_path, text_types, max_tokens, client, collection_name, embedding_model, payload_indexing)\u001b[0m\n\u001b[1;32m    194\u001b[0m     client \u001b[39m=\u001b[39m QdrantClient()\n\u001b[1;32m    195\u001b[0m     client\u001b[39m.\u001b[39mset_model(embedding_model)\n\u001b[0;32m--> 197\u001b[0m texts \u001b[39m=\u001b[39m get_files_from_dir(dir_path, text_types, recursive\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    198\u001b[0m chunks \u001b[39m=\u001b[39m split_files_to_chunks(texts, max_tokens)\n\u001b[1;32m    199\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFound \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(chunks)\u001b[39m}\u001b[39;00m\u001b[39m chunks.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/local_projects/claude_test/qdrant_retrieval.py:113\u001b[0m, in \u001b[0;36msplit_files_to_chunks\u001b[0;34m(files, max_tokens)\u001b[0m\n\u001b[1;32m    110\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo text available in file: \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m         \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Skip to the next file if no text is available\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     chunks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m split_text_to_chunks(text, max_tokens)\n\u001b[1;32m    115\u001b[0m \u001b[39mreturn\u001b[39;00m chunks\n",
            "File \u001b[0;32m~/Documents/local_projects/claude_test/qdrant_retrieval.py:57\u001b[0m, in \u001b[0;36msplit_text_to_chunks\u001b[0;34m(text, max_tokens, overlap)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m cnt \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     53\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m     54\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_tokens is too small to fit a single line of text. Breaking this line:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00mlines[\u001b[39m0\u001b[39m][:\u001b[39m100\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m ...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m     )\n\u001b[0;32m---> 57\u001b[0m split_len \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(max_tokens \u001b[39m/\u001b[39;49m lines_tokens[\u001b[39m0\u001b[39;49m] \u001b[39m*\u001b[39m \u001b[39m0.9\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(lines[\u001b[39m0\u001b[39m]))\n\u001b[1;32m     58\u001b[0m prev \u001b[39m=\u001b[39m lines[\u001b[39m0\u001b[39m][:split_len]\n\u001b[1;32m     59\u001b[0m lines[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m lines[\u001b[39m0\u001b[39m][split_len:]\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ],
      "source": [
        "create_qdrant_collection_from_dir(\n",
        "    dir_path=[\"json_files/test.json\"],\n",
        "    text_types=[\"json\"],\n",
        "    collection_name=\"json_collection\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b36bddd6",
      "metadata": {
        "id": "b36bddd6"
      },
      "source": [
        "## Retrieval Augmented Generation\n",
        "\n",
        "RAG changes the way we interact with Large Language Models. We're converting a knowledge-oriented task, in which the model may create a counterfactual answer, into a language-oriented task. The latter expects the model to extract meaningful information and generate an answer. LLMs, when implemented correctly, are supposed to be carrying out language-oriented tasks.\n",
        "\n",
        "The task starts with the original prompt sent by the user. The same prompt is then vectorized and used as a search query for the most relevant facts. Those facts are combined with the original prompt to build a longer prompt containing more information.\n",
        "\n",
        "But let's start simply by asking our question directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ed31ca63",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:36.695165Z",
          "start_time": "2023-09-27T10:06:36.695150Z"
        },
        "id": "ed31ca63"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "What tools should I need to use to build a web service using vector embeddings for search?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d2d7dd",
      "metadata": {
        "id": "e7d2d7dd"
      },
      "source": [
        "Using OpenAI API requires providing the API key. Our example demonstrates setting the `OPENAI_API_KEY` using an environmental variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "30e8669e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:36.696985Z",
          "start_time": "2023-09-27T10:06:36.696959Z"
        },
        "id": "30e8669e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Fill the environmental variable with your own OpenAI API key\n",
        "# See: https://platform.openai.com/account/api-keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<< PASS YOUR OWN KEY >>\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bf5c684",
      "metadata": {
        "id": "7bf5c684"
      },
      "source": [
        "Now we can finally call the completion service."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5cdee82",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:36.700541Z",
          "start_time": "2023-09-27T10:06:36.700518Z"
        },
        "id": "d5cdee82",
        "outputId": "458f1739-dad1-43e0-98a4-e3fe7d9109e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To build a web service using vector embeddings for search, you would need several tools. Here are some essential ones:\n",
            "\n",
            "1. Programming Language: Depending on your preference and requirements, you can choose a programming language like Python, Java, or Node.js for building the web service.\n",
            "\n",
            "2. Web Framework: A web framework helps in developing web applications efficiently. Popular choices include Flask (Python) or Spring Boot (Java).\n",
            "\n",
            "3. Embedding Models: You would need vector embedding models to represent your data for search. Some popular models include Word2Vec, GloVe, or BERT, depending on your application domain and requirements.\n",
            "\n",
            "4. Vector Embedding Libraries: To work with vector embeddings effectively, you may need libraries like TensorFlow, Gensim, or PyTorch to load and manipulate the embeddings.\n",
            "\n",
            "5. Database: You would require a database to store and retrieve the information you want to search. Common choices include MySQL, PostgreSQL, or MongoDB.\n",
            "\n",
            "6. Search Engine: You need a tool or library to enable efficient searching using vector embeddings. Tools like Elasticsearch or Apache Solr can be integrated to perform vector similarity searches.\n",
            "\n",
            "7. API Development: To expose your web service functionality, you may need to develop an API. Popular frameworks like Flask or Django (Python) or Express (Node.js) can assist in creating the API endpoints.\n",
            "\n",
            "8. Deployment Environment: You would need a hosting environment to deploy your web service. Options include cloud platforms such as Amazon Web Services (AWS), Google Cloud, or Microsoft Azure, or you can choose a self-hosted solution.\n",
            "\n",
            "Remember to choose the tools that best suit your requirements, taking into account factors like programming language proficiency, scalability, and performance needs.\n"
          ]
        }
      ],
      "source": [
        "# import openai\n",
        "\n",
        "# completion = openai.ChatCompletion.create(\n",
        "#     model=\"gpt-3.5-turbo\",\n",
        "#     messages=[\n",
        "#         {\"role\": \"user\", \"content\": prompt},\n",
        "#     ]\n",
        "# )\n",
        "# print(completion[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4084aa45",
      "metadata": {},
      "source": [
        "### Claude alternative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3be6f5a4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ContentBlock(text='To build a web service using vector embeddings for search, need the following tools, you will:\\n\\n1. A programming language, such as Python or Java, choose you must. Popular for machine learning and natural language processing, Python is.\\n\\n2. A web framework like Flask or Django, select you should. Easy to create web services and APIs, these frameworks make.\\n\\n3. A library for generating vector embeddings, require you will. Popular options include TensorFlow, PyTorch, or Gensim. Powerful and flexible, these libraries are.\\n\\n4. A database to store your vector embeddings and search indices, consider you must. Options like Elasticsearch, Faiss, or Annoy, explore you can. Efficient storage and retrieval of high-dimensional vectors, these databases provide.\\n\\n5. A machine learning model for generating vector embeddings, train or obtain you must. Pre-trained models like Word2Vec, Glo', type='text')]\n"
          ]
        }
      ],
      "source": [
        "import anthropic\n",
        "\n",
        "API_KEY = \"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\"\n",
        "\n",
        "\n",
        "client = anthropic.Anthropic(\n",
        "    #api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n",
        "    api_key=API_KEY\n",
        ")\n",
        "\n",
        "message = client.messages.create(\n",
        "    model=\"claude-3-opus-20240229\",\n",
        "    #model=\"claude-2.1\",\n",
        "    max_tokens=200,\n",
        "    temperature=0.0,\n",
        "    system=\"Respond only in Yoda-speak.\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b420d81d",
      "metadata": {
        "id": "b420d81d"
      },
      "source": [
        "### Extending the prompt\n",
        "\n",
        "Even though the original answer sounds credible, it didn't answer our question correctly. Instead, it gave us a generic description of an application stack. To improve the results, enriching the original prompt with the descriptions of the tools available seems like one of the possibilities. Let's use a semantic knowledge base to augment the prompt with the descriptions of different technologies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ce791ba3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:36.702641Z",
          "start_time": "2023-09-27T10:06:36.702619Z"
        },
        "id": "ce791ba3",
        "outputId": "eab4ba26-2233-45e0-8c23-a9e965923efe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[QueryResponse(id='0e25a5b2-9fc8-4ba5-904d-7535cfa32e7f', embedding=None, sparse_embedding=None, metadata={'document': 'Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!'}, document='Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!', score=0.8290700912475586),\n",
              " QueryResponse(id='69a21a6c-4ed4-49f3-86ce-2ff957b19052', embedding=None, sparse_embedding=None, metadata={'document': 'Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!'}, document='Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!', score=0.8290700912475586),\n",
              " QueryResponse(id='0606d89c-2f97-4b39-bf0d-b4723d3766c2', embedding=None, sparse_embedding=None, metadata={'document': 'FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.'}, document='FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.', score=0.8190129399299622)]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = qdrant_client.query(\n",
        "    collection_name=\"knowledge-base\",\n",
        "    query_text=prompt,\n",
        "    limit=3,\n",
        ")\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6640067",
      "metadata": {
        "id": "c6640067"
      },
      "source": [
        "We used the original prompt to perform a semantic search over the set of tool descriptions. Now we can use these descriptions to augment the prompt and create more context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a16d8549",
      "metadata": {
        "id": "a16d8549",
        "outputId": "9e325c31-4b07-4c14-a304-bfc1dab2e571"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\\nQdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\\nFastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context = \"\\n\".join(r.document for r in results)\n",
        "context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2c04a4e",
      "metadata": {
        "id": "a2c04a4e"
      },
      "source": [
        "Finally, let's build a metaprompt, the combination of the assumed role of the LLM, the original question, and the results from our semantic search that will force our LLM to use the provided context.\n",
        "\n",
        "By doing this, we effectively convert the knowledge-oriented task into a language task and hopefully reduce the chances of hallucinations. It also should make the response sound more relevant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1fc9a98b",
      "metadata": {
        "id": "1fc9a98b",
        "outputId": "aff01a1f-a493-44ef-ddb4-7ce1a3ee7511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You are a software architect.\n",
            "Answer the following question using the provided context.\n",
            "If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
            "\n",
            "Question: What tools should I need to use to build a web service using vector embeddings for search?\n",
            "\n",
            "Context:\n",
            "Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\n",
            "Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\n",
            "FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\n",
            "\n",
            "Answer:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "metaprompt = f\"\"\"\n",
        "You are a software architect.\n",
        "Answer the following question using the provided context.\n",
        "If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
        "\n",
        "Question: {prompt.strip()}\n",
        "\n",
        "Context:\n",
        "{context.strip()}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Look at the full metaprompt\n",
        "print(metaprompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f1a7678",
      "metadata": {
        "id": "9f1a7678"
      },
      "source": [
        "Our current prompt is much longer, and we also used a couple of strategies to make the responses even better:\n",
        "\n",
        "1. The LLM has the role of software architect.\n",
        "2. We provide more context to answer the question.\n",
        "3. If the context contains no meaningful information, the model shouldn't make up an answer.\n",
        "\n",
        "Let's find out if that works as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "709b9f38",
      "metadata": {
        "id": "709b9f38",
        "outputId": "cf6ac644-7d73-42df-b72a-e5d725d43da5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To build a web service using vector embeddings for search, you would need to use Qdrant as the vector database and vector similarity search engine. Additionally, you can use FastAPI as the web framework for building the API. To compute the vector embeddings, you can utilize SentenceTransformers, which is a Python framework for generating sentence/text embeddings. These tools would enable you to create a web service for matching, searching, recommending, and more based on vector embeddings.\n"
          ]
        }
      ],
      "source": [
        "# completion = openai.ChatCompletion.create(\n",
        "#     model=\"gpt-3.5-turbo\",\n",
        "#     messages=[\n",
        "#         {\"role\": \"user\", \"content\": metaprompt},\n",
        "#     ],\n",
        "#     timeout=10.0,\n",
        "# )\n",
        "# print(completion[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e334e248",
      "metadata": {},
      "source": [
        "### Claude alternative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3467f35a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ContentBlock(text='Based on the provided context, you can use the following tools to build a web service using vector embeddings for search:\\n\\n1. Qdrant: Qdrant is a vector database and vector similarity search engine that can be deployed as an API service. It allows you to store and search for high-dimensional vectors, making it suitable for applications like matching, searching, and recommending based on vector embeddings.\\n\\n2. FastAPI: FastAPI is a modern web framework for building APIs with Python. It provides a fast and efficient way to create a web service that can interact with the Qdrant vector database. You can use FastAPI to define API endpoints, handle requests, and process responses.\\n\\nBy combining Qdrant and FastAPI, you can create a web service that utilizes vector embeddings for search functionality. Qdrant will handle the storage and retrieval of vector embeddings, while FastAPI will provide the API interface to interact with', type='text')]\n"
          ]
        }
      ],
      "source": [
        "message = client.messages.create(\n",
        "    model=\"claude-3-opus-20240229\",\n",
        "    #model=\"claude-2.1\",\n",
        "    max_tokens=200,\n",
        "    temperature=0.0,\n",
        "    #system=\"Respond only in Yoda-speak.\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": metaprompt}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4120e1-9899-4caa-b974-51d9b3a485be",
      "metadata": {
        "id": "1c4120e1-9899-4caa-b974-51d9b3a485be"
      },
      "source": [
        "### Testing out the RAG pipeline\n",
        "\n",
        "By leveraging the semantic context we provided our model is doing a better job answering the question. Let's enclose the RAG as a function, so we can call it more easily for different prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62ed09d1-2c90-4ffc-9f1d-7beb87bab78b",
      "metadata": {
        "id": "62ed09d1-2c90-4ffc-9f1d-7beb87bab78b"
      },
      "outputs": [],
      "source": [
        "# def rag(question: str, n_points: int = 3) -> str:\n",
        "#     results = client.query(\n",
        "#         collection_name=\"knowledge-base\",\n",
        "#         query_text=question,\n",
        "#         limit=n_points,\n",
        "#     )\n",
        "\n",
        "#     context = \"\\n\".join(r.document for r in results)\n",
        "\n",
        "#     metaprompt = f\"\"\"\n",
        "#     You are a software architect.\n",
        "#     Answer the following question using the provided context.\n",
        "#     If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
        "\n",
        "#     Question: {question.strip()}\n",
        "\n",
        "#     Context:\n",
        "#     {context.strip()}\n",
        "\n",
        "#     Answer:\n",
        "#     \"\"\"\n",
        "\n",
        "#     completion = openai.ChatCompletion.create(\n",
        "#         model=\"gpt-3.5-turbo\",\n",
        "#         messages=[\n",
        "#             {\"role\": \"user\", \"content\": metaprompt},\n",
        "#         ],\n",
        "#         timeout=10.0,\n",
        "#     )\n",
        "#     return completion[\"choices\"][0][\"message\"][\"content\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e10b5fce",
      "metadata": {},
      "source": [
        "### Claude alternative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "947d208e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rag(question: str, n_points: int = 3) -> str:\n",
        "    results = qdrant_client.query(\n",
        "        collection_name=\"knowledge-base\",\n",
        "        query_text=question,\n",
        "        limit=n_points,\n",
        "    )\n",
        "\n",
        "    context = \"\\n\".join(r.document for r in results)\n",
        "\n",
        "    metaprompt = f\"\"\"\n",
        "    You are a software architect.\n",
        "    Answer the following question using the provided context.\n",
        "    If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
        "\n",
        "    Question: {question.strip()}\n",
        "\n",
        "    Context:\n",
        "    {context.strip()}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    message = client.messages.create(\n",
        "        model=\"claude-3-opus-20240229\",\n",
        "        #model=\"claude-2.1\",\n",
        "        max_tokens=200,\n",
        "        temperature=0.0,\n",
        "        #system=\"Respond only in Yoda-speak.\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": metaprompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86fecd76-9a0b-4ad1-9097-b1d292a618ac",
      "metadata": {
        "id": "86fecd76-9a0b-4ad1-9097-b1d292a618ac"
      },
      "source": [
        "Now it's easier to ask a broad range of questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "aa0fdead-a115-4fcd-88dc-5cc718dc0544",
      "metadata": {
        "id": "aa0fdead-a115-4fcd-88dc-5cc718dc0544",
        "outputId": "d1ef512e-4a06-4da2-fec9-2d5d48b2c61b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ContentBlock(text='Based on the provided context, a possible stack for a web API can include:\\n\\n1. FastAPI: A modern, fast, and high-performance web framework for building APIs using Python 3.7+. FastAPI leverages Python type hints and provides automatic API documentation and validation.\\n\\n2. Python 3.7+: The programming language used to develop the web API, taking advantage of modern Python features and syntax.\\n\\n3. Docker: A containerization platform that allows developers to package the web API along with its dependencies into a container. Docker enables easy deployment, scalability, and portability of the application across different environments without the need for tedious environment configuration or management.\\n\\nSo, a typical stack for a web API can consist of using FastAPI as the web framework, Python 3.7+ as the programming language, and Docker for containerization and deployment. This stack provides a fast, efficient, and scalable solution for building and deploying web APIs', type='text')]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag(\"What can the stack for a web api look like?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7324c127-c140-410a-ab19-87a5babce023",
      "metadata": {
        "id": "7324c127-c140-410a-ab19-87a5babce023",
        "outputId": "4a0771b3-5a3e-48b8-84ad-c28039106e5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ContentBlock(text=\"I don't know.\", type='text')]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag(\"Where is the nearest grocery store?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fe56730-ed41-42c1-9c33-de3849c60b65",
      "metadata": {
        "id": "6fe56730-ed41-42c1-9c33-de3849c60b65"
      },
      "source": [
        "Our model can now:\n",
        "\n",
        "1. Take advantage of the knowledge in our vector datastore.\n",
        "2. Answer, based on the provided context, that it can not provide an answer.\n",
        "\n",
        "We have just shown a useful mechanism to mitigate the risks of hallucinations in Large Language Models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8bed99a",
      "metadata": {},
      "source": [
        "### Create Qdrant collection from dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d16282b1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ids': [['0606d89c-2f97-4b39-bf0d-b4723d3766c2', 'e96afda8-c213-4744-9270-bcc20f534692', '415842ee-83b7-442f-88da-9e2ed19c0265', '65c8a7fd-2f1a-4afa-96d4-32ee333101ce', '84f55a93-25c8-4aba-b0db-faf2dfbf675d', '2420b07b-907e-47cb-98af-d64ee5b30774', 'e271d182-86de-4f5f-857e-f3e72791beba', 'a952fd65-8c84-4819-9a91-ef90bd07ad03', '94cedf9f-528d-4531-bcfd-6fbf013d127f', '94d083c8-26bb-4e87-b679-f766993bfbcc', 'bd2e224f-66b7-4770-acd5-b3c0e692fa6b', 'bcaf6263-71b2-4898-b53d-a2dd1994cf89', '0e25a5b2-9fc8-4ba5-904d-7535cfa32e7f', '69a21a6c-4ed4-49f3-86ce-2ff957b19052', '3e72bb6e-c551-4a42-89bc-ad8ad8d66615', '7fb937a9-1fda-4e08-92fc-023bc5eba212'], ['0606d89c-2f97-4b39-bf0d-b4723d3766c2', 'e96afda8-c213-4744-9270-bcc20f534692', '415842ee-83b7-442f-88da-9e2ed19c0265', '65c8a7fd-2f1a-4afa-96d4-32ee333101ce', '84f55a93-25c8-4aba-b0db-faf2dfbf675d', '2420b07b-907e-47cb-98af-d64ee5b30774', 'e271d182-86de-4f5f-857e-f3e72791beba', 'a952fd65-8c84-4819-9a91-ef90bd07ad03', '94cedf9f-528d-4531-bcfd-6fbf013d127f', '94d083c8-26bb-4e87-b679-f766993bfbcc', 'bd2e224f-66b7-4770-acd5-b3c0e692fa6b', 'bcaf6263-71b2-4898-b53d-a2dd1994cf89', '0e25a5b2-9fc8-4ba5-904d-7535cfa32e7f', '69a21a6c-4ed4-49f3-86ce-2ff957b19052', '3e72bb6e-c551-4a42-89bc-ad8ad8d66615', '7fb937a9-1fda-4e08-92fc-023bc5eba212']], 'documents': [['FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.', 'FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.', 'PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.', 'PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.', 'Docker helps developers build, share, and run applications anywhere — without tedious environment configuration or management.', 'Docker helps developers build, share, and run applications anywhere — without tedious environment configuration or management.', 'SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.', 'SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.', 'The cron command-line utility is a job scheduler on Unix-like operating systems. Users who set up and maintain software environments use cron to schedule jobs (commands or shell scripts), also known as cron jobs, to run periodically at fixed times, dates, or intervals.', 'The cron command-line utility is a job scheduler on Unix-like operating systems. Users who set up and maintain software environments use cron to schedule jobs (commands or shell scripts), also known as cron jobs, to run periodically at fixed times, dates, or intervals.', 'MySQL is an open-source relational database management system (RDBMS). A relational database organizes data into one or more data tables in which data may be related to each other; these relations help structure the data. SQL is a language that programmers use to create, modify and extract data from the relational database, as well as control user access to the database.', 'MySQL is an open-source relational database management system (RDBMS). A relational database organizes data into one or more data tables in which data may be related to each other; these relations help structure the data. SQL is a language that programmers use to create, modify and extract data from the relational database, as well as control user access to the database.', 'Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!', 'Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!', 'NGINX is a free, open-source, high-performance HTTP server and reverse proxy, as well as an IMAP/POP3 proxy server. NGINX is known for its high performance, stability, rich feature set, simple configuration, and low resource consumption.', 'NGINX is a free, open-source, high-performance HTTP server and reverse proxy, as well as an IMAP/POP3 proxy server. NGINX is known for its high performance, stability, rich feature set, simple configuration, and low resource consumption.'], ['FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.', 'FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.', 'PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.', 'PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.', 'Docker helps developers build, share, and run applications anywhere — without tedious environment configuration or management.', 'Docker helps developers build, share, and run applications anywhere — without tedious environment configuration or management.', 'SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.', 'SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.', 'The cron command-line utility is a job scheduler on Unix-like operating systems. Users who set up and maintain software environments use cron to schedule jobs (commands or shell scripts), also known as cron jobs, to run periodically at fixed times, dates, or intervals.', 'The cron command-line utility is a job scheduler on Unix-like operating systems. Users who set up and maintain software environments use cron to schedule jobs (commands or shell scripts), also known as cron jobs, to run periodically at fixed times, dates, or intervals.', 'MySQL is an open-source relational database management system (RDBMS). A relational database organizes data into one or more data tables in which data may be related to each other; these relations help structure the data. SQL is a language that programmers use to create, modify and extract data from the relational database, as well as control user access to the database.', 'MySQL is an open-source relational database management system (RDBMS). A relational database organizes data into one or more data tables in which data may be related to each other; these relations help structure the data. SQL is a language that programmers use to create, modify and extract data from the relational database, as well as control user access to the database.', 'Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!', 'Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!', 'NGINX is a free, open-source, high-performance HTTP server and reverse proxy, as well as an IMAP/POP3 proxy server. NGINX is known for its high performance, stability, rich feature set, simple configuration, and low resource consumption.', 'NGINX is a free, open-source, high-performance HTTP server and reverse proxy, as well as an IMAP/POP3 proxy server. NGINX is known for its high performance, stability, rich feature set, simple configuration, and low resource consumption.']]}\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(os.getcwd() + \"/qdrant_retrieval.py\")\n",
        "import qdrant_retrieval\n",
        "from qdrant_client import QdrantClient\n",
        "import importlib\n",
        "importlib.reload(qdrant_retrieval)\n",
        "from qdrant_retrieval import create_qdrant_collection_from_dir, retrieve_docs\n",
        "# create_qdrant_collection_from_dir(\"https://raw.githubusercontent.com/microsoft/flaml/main/README.md\",\n",
        "#         \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\")\n",
        "res = retrieve_docs(query_texts=\"Is there a function called tune_automl?\", client=QdrantClient(), collection_name=\"knowledge-base\")\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa8c23c7",
      "metadata": {},
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d9dde6c",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "968a3ae6",
      "metadata": {
        "id": "968a3ae6"
      },
      "source": [
        "### Cleaning up the environment\n",
        "\n",
        "If you wish to continue playing with the RAG application we created, don't do the code below. However, it's always good to clean up the environment, so nothing is left dangling. We'll show you how to remove the Qdrant container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0729043",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:36.704761Z",
          "start_time": "2023-09-27T10:06:36.704742Z"
        },
        "id": "a0729043"
      },
      "outputs": [],
      "source": [
        "!docker kill rag-openai-qdrant\n",
        "!docker rm rag-openai-qdrant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97bb6ed9",
      "metadata": {
        "id": "97bb6ed9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.8 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "464e4de32d4d2b9d4b4a3d5f9b379d2e2438d041f1fbd0e9d1088d8523068195"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
